{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available!\n"
     ]
    }
   ],
   "source": [
    "# 检查 MPS 是否可用\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS is available!\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS is not available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词表生成模块\n",
    "目标：给每个n-gram切片一个独有的索引id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 将单词、句子转化为n-gram切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#he', 'hel', 'ell', 'llo', 'low', 'owo', 'wor', 'orl', 'rld', 'ld#']\n",
      "['#he', 'hel', 'ell', 'llo', 'lo#', '#wo', 'wor', 'orl', 'rld', 'ld#']\n"
     ]
    }
   ],
   "source": [
    "def n_gram(word, n=3):\n",
    "    s = []\n",
    "    word = '#' + word + '#'\n",
    "    for i in range(len(word) - n + 1):\n",
    "        s.append(word[i:i + n])\n",
    "    return s\n",
    "\n",
    "def lst_gram(lst, n=3):\n",
    "    s = []\n",
    "    for word in str(lst).lower().split():\n",
    "        s.extend(n_gram(word, n))\n",
    "    return s \n",
    "\n",
    "print(n_gram('helloworld'))\n",
    "print(lst_gram('hello world'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取文件\n",
    "### 遍历文件中出现的每一个词，将其转化为n-gram切片后加入列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/'\n",
    "files = ['norm_train_claims.json', 'norm_dev_claims.json','norm_climate_evidences.json']\n",
    "\n",
    "for file in files:\n",
    "    with open(file_path + file, 'r', encoding='utf-8') as f:\n",
    "        if file == 'norm_train_claims.json':\n",
    "            train_claims = json.load(f)\n",
    "        elif file == 'norm_dev_claims.json':\n",
    "            dev_claims = json.load(f)\n",
    "        elif file == 'norm_climate_evidences.json':\n",
    "            evidence = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in datasets: 461678\n"
     ]
    }
   ],
   "source": [
    "unique_words = []\n",
    "for key, value in train_claims.items():\n",
    "    unique_words.extend(value['norm_claim'].split())\n",
    "for key, value in dev_claims.items():\n",
    "    unique_words.extend(value['norm_claim'].split())\n",
    "for key, value in evidence.items():\n",
    "    unique_words.extend(value.split())\n",
    "unique_words = list(set(unique_words))\n",
    "print(\"Number of unique words in datasets:\", len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lst_gram' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m vocab \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m train_claims\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 4\u001b[0m     vocab\u001b[38;5;241m.\u001b[39mextend(lst_gram(value[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorm_claim\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m      5\u001b[0m     evid_ids \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevidences\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m evid_id \u001b[38;5;129;01min\u001b[39;00m evid_ids:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lst_gram' is not defined"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "\n",
    "for key, value in train_claims.items():\n",
    "    vocab.extend(lst_gram(value['norm_claim']))\n",
    "    evid_ids = value['evidences']\n",
    "    for evid_id in evid_ids:\n",
    "        if evid_id in evidence.keys():\n",
    "            vocab.extend(lst_gram(evidence[evid_id]))\n",
    "for key, value in dev_claims.items():\n",
    "    vocab.extend(lst_gram(value['norm_claim']))\n",
    "    evid_ids = value['evidences']\n",
    "    for evid_id in evid_ids:\n",
    "        if evid_id in evidence.keys():\n",
    "            vocab.extend(lst_gram(evidence[evid_id]))\n",
    "# for key, value in evidence.items():\n",
    "#     vocab.extend(lst_gram(value))\n",
    "\n",
    "### 去重\n",
    "vocab = list(set(vocab))\n",
    "print(\"Number of unique vocab in datasets:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加特殊字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final vocab size: 6131\n"
     ]
    }
   ],
   "source": [
    "vocab_list = ['[PAD]', '[UNK]']\n",
    "vocab_list.extend(vocab)\n",
    "print(\"Final vocab size:\", len(vocab_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/vocab_true_evid.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in vocab_list:\n",
    "        f.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "evid_text = list(evidence.values())\n",
    "evid_text_test = evid_text[0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定义哈希空间维度（如 10000 或更大，视内存和冲突情况而定）\n",
    "HASH_SPACE_DIM = 10000\n",
    "\n",
    "def hashing_trick(word, hash_space_dim=HASH_SPACE_DIM):\n",
    "    \"\"\"\n",
    "    将单个词转换为哈希索引，使用 hash 函数并对哈希空间维度取模。\n",
    "    \"\"\"\n",
    "    return hash(word) % hash_space_dim\n",
    "\n",
    "def vocab_to_hash(vocab_list, hash_space_dim=HASH_SPACE_DIM):\n",
    "    \"\"\"\n",
    "    对整个 vocab list 进行哈希映射，返回哈希索引列表。\n",
    "    \"\"\"\n",
    "    hashed_vocab = [hashing_trick(word, hash_space_dim) for word in vocab_list]\n",
    "    return hashed_vocab\n",
    "\n",
    "def evidence_to_embedding(evidence, vocab_list, hash_space_dim=HASH_SPACE_DIM, embedding_dim=100):\n",
    "    \"\"\"\n",
    "    对每个 evidence 进行 n-gram 分词，将 n-gram 词映射到哈希索引，并生成 embedding。\n",
    "    \"\"\"\n",
    "    # 将 vocab list 转化为哈希索引列表\n",
    "    hashed_vocab = vocab_to_hash(vocab_list, hash_space_dim)\n",
    "\n",
    "    # 使用分词和哈希结果初始化 embedding\n",
    "    embedding = np.zeros(hash_space_dim)\n",
    "\n",
    "    # 对每个 n-gram 词应用哈希映射，更新对应位置的值\n",
    "    for word in evidence:\n",
    "        hash_index = hashing_trick(word, hash_space_dim)\n",
    "        embedding[hash_index] += 1  # 累计出现次数，可以用 TF-IDF 或其他权重替代\n",
    "\n",
    "    return embedding\n",
    "\n",
    "# 示例：假设我们有一个 vocab list 和 evidence 列表\n",
    "vocab_list = [\"word1\", \"word2\", \"example\", \"ngram\", \"sample\"]\n",
    "evidence = [\"example\", \"word2\", \"ngram\", \"example\"]\n",
    "\n",
    "# 对 evidence 生成 embedding\n",
    "embedding = evidence_to_embedding(evidence, vocab_list)\n",
    "print(\"Evidence Embedding:\\n\", embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "储存到文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file_path = './data/vocab_with_evid.txt'\n",
    "with open(vocab_file_path, 'w', encoding='utf-8') as f:\n",
    "    for slice in vocab_list:\n",
    "        f.write(slice + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU 版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashing_trick(word, HASH_SPACE_DIM= 2000):\n",
    "    return hash(word) % HASH_SPACE_DIM\n",
    "\n",
    "def vocab_to_hash(vocab_path, HASH_SPACE_DIM = 2000):\n",
    "    vocab_list = open(vocab_path, encoding='utf-8').readlines()\n",
    "    # hashed_vocab = [hashing_trick(word, hash_space_dim) for word in vocab_list]\n",
    "    hashed_vocab = {word: hashing_trick(word, HASH_SPACE_DIM) for word in vocab_list}\n",
    "    return hashed_vocab\n",
    "\n",
    "def n_gram_gpu(text_tensor, n=3):\n",
    "    ngrams = [text_tensor[i:i+n] for i in range(len(text_tensor) - n + 1)]\n",
    "    return ngrams\n",
    "    \n",
    "def get_batch(evid_text, batch_size=1024, start_index = 0):\n",
    "    end_index = start_index + batch_size\n",
    "    batch_data = evid_text[start_index:end_index]\n",
    "    return batch_data, end_index\n",
    "\n",
    "def process_batch_char(batch_data, indices_tensor, HASH_SPACE_DIM, n_gram = 3, max_length = 100):\n",
    "    batch_char_list = []\n",
    "    for evidence in batch_data:\n",
    "        text_tensor = torch.tensor([ord(c) for c in evidence]).to(\"mps\")\n",
    "        ngrams = n_gram_gpu(text_tensor, n_gram)\n",
    "        \n",
    "        ngram_hashes = torch.tensor([hash(ngram) % HASH_SPACE_DIM for ngram in ngrams], dtype=torch.long).to(\"mps\")\n",
    "        hash_indices = indices_tensor[ngram_hashes]\n",
    "        batch_char_list.append(hash_indices)\n",
    "    \n",
    "        # 在 GPU 上填充序列\n",
    "    batch_char_tensor = torch.nn.utils.rnn.pad_sequence(batch_char_list, batch_first=True, padding_value=0).to(\"mps\")\n",
    "    \n",
    "    # 截断或补充到 max_length\n",
    "    if batch_char_tensor.size(1) > max_length:\n",
    "        # 截断\n",
    "        batch_char_tensor = batch_char_tensor[:, :max_length]\n",
    "    else:\n",
    "        # 补充\n",
    "        padding_size = max_length - batch_char_tensor.size(1)\n",
    "        batch_char_tensor = F.pad(batch_char_tensor, (0, padding_size), value=0)\n",
    "    \n",
    "    return batch_char_tensor\n",
    "\n",
    "def load_evd_text(vocab_path, evid_text,HASH_SPACE_DIM = 2000, n_gram = 3, max_length = 100):\n",
    "    hashed_vocab = vocab_to_hash(vocab_path = vocab_path,\n",
    "                            HASH_SPACE_DIM = HASH_SPACE_DIM)\n",
    "    ngrams_list = list(hashed_vocab.keys())  # 获取所有 n-gram 作为列表\n",
    "    indices_list = list(hashed_vocab.values())  # 获取对应的哈希索引列表\n",
    "\n",
    "    # 将 n-grams 和索引转化为 GPU 上的张量\n",
    "    ngrams_tensor = torch.tensor([hash(ngram) % HASH_SPACE_DIM for ngram in ngrams_list]).to(\"mps\")\n",
    "    indices_tensor = torch.tensor(indices_list).to(\"mps\")\n",
    "\n",
    "    start_index = 0\n",
    "    batch_size = 49152\n",
    "    all_evd_chars = []\n",
    "    while start_index < len(evid_text):\n",
    "        print(\"current processed batch index\",start_index)\n",
    "        batch_data, start_index = get_batch(evid_text, batch_size, start_index)\n",
    "        batch_char_list = process_batch_char(batch_data, indices_tensor, HASH_SPACE_DIM, n_gram = 3, max_length = 100)\n",
    "        batch_char_tensor_cpu = batch_char_list.to(\"cpu\")\n",
    "        all_evd_chars.append(batch_char_tensor_cpu)\n",
    "    return all_evd_chars\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashed_vocab = vocab_to_hash(vocab_path = './data/vocab_true_evid.txt',\n",
    "                            hash_space_dim = 2000)\n",
    "ngrams_list = list(hashed_vocab.keys())  # 获取所有 n-gram 作为列表\n",
    "indices_list = list(hashed_vocab.values())  # 获取对应的哈希索引列表\n",
    "\n",
    "# 将 n-grams 和索引转化为 GPU 上的张量\n",
    "ngrams_tensor = torch.tensor([hash(ngram) % vocab_size for ngram in ngrams_list]).to(\"mps\")\n",
    "indices_tensor = torch.tensor(indices_list).to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_char(batch_data, indices_tensor, HASH_SPACE_DIM, n_gram = 3, max_length = 100):\n",
    "    batch_char_list = []\n",
    "    for evidence in batch_data:\n",
    "        text_tensor = torch.tensor([ord(c) for c in evidence]).to(\"mps\")\n",
    "        ngrams = n_gram_gpu(text_tensor, n_gram)\n",
    "        \n",
    "        ngram_hashes = torch.tensor([hash(ngram) % HASH_SPACE_DIM for ngram in ngrams], dtype=torch.long).to(\"mps\")\n",
    "        hash_indices = indices_tensor[ngram_hashes]\n",
    "        batch_char_list.append(hash_indices)\n",
    "    \n",
    "        # 在 GPU 上填充序列\n",
    "    batch_char_tensor = torch.nn.utils.rnn.pad_sequence(batch_char_list, batch_first=True, padding_value=0).to(\"mps\")\n",
    "    \n",
    "    # 截断或补充到 max_length\n",
    "    if batch_char_tensor.size(1) > max_length:\n",
    "        # 截断\n",
    "        batch_char_tensor = batch_char_tensor[:, :max_length]\n",
    "    else:\n",
    "        # 补充\n",
    "        padding_size = max_length - batch_char_tensor.size(1)\n",
    "        batch_char_tensor = F.pad(batch_char_tensor, (0, padding_size), value=0)\n",
    "    \n",
    "    return batch_char_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_claim_text_to_char(vocab_path, claim_df, HASH_SPACE_DIM, n_gram = 3, max_length = 100):\n",
    "    hashed_vocab = vocab_to_hash(vocab_path = vocab_path,\n",
    "                            HASH_SPACE_DIM = HASH_SPACE_DIM)    \n",
    "    ngrams_list = list(hashed_vocab.keys())  # 获取所有 n-gram 作为列表\n",
    "    indices_list = list(hashed_vocab.values())  # 获取对应的哈希索引列表\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    # 将 n-grams 和索引转化为 GPU 上的张量\n",
    "    ngrams_tensor = torch.tensor([hash(ngram) % HASH_SPACE_DIM for ngram in ngrams_list]).to(\"mps\")\n",
    "    indices_tensor = torch.tensor(indices_list).to(\"mps\")\n",
    "    all_char_list = []\n",
    "    for claim_id, values in claim_df.items():\n",
    "        claim_text = values['norm_claim']\n",
    "        text_tensor = torch.tensor([ord(c) for c in claim_text]).to(\"mps\")\n",
    "        ngrams = n_gram_gpu(text_tensor, n_gram)\n",
    "\n",
    "        ngram_hashes = torch.tensor([hash(ngram) % HASH_SPACE_DIM for ngram in ngrams], dtype=torch.long).to(\"mps\")\n",
    "        hash_indices = indices_tensor[ngram_hashes]\n",
    "\n",
    "        all_char_list.append(hash_indices)\n",
    "\n",
    "        # 在 GPU 上填充序列\n",
    "    all_char_list = torch.nn.utils.rnn.pad_sequence(all_char_list, batch_first=True, padding_value=0).to(\"mps\")\n",
    "\n",
    "    # 截断或补充到 max_length\n",
    "    if all_char_list.size(1) > max_length:\n",
    "        # 截断\n",
    "        all_char_list = all_char_list[:, :max_length]\n",
    "    else:\n",
    "        # 补充\n",
    "        padding_size = max_length - all_char_list.size(1)\n",
    "        all_char_list = F.pad(all_char_list, (0, padding_size), value=0)\n",
    "\n",
    "    return all_char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1228"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_char_list = load_claim_text_to_char(vocab_path = './data/vocab_true_evid.txt', \n",
    "                                        claim_df = train_claims, HASH_SPACE_DIM=2000, n_gram = 3, max_length = 100)\n",
    "len(all_char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_char = load_claim_text_to_char(vocab_path = './data/vocab_true_evid.txt',\n",
    "                                        claim_df = dev_claims, HASH_SPACE_DIM=2000, n_gram = 3, max_length = 100)\n",
    "len(dev_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evd_text(vocab_path, evid_text,HASH_SPACE_DIM = 2000, n_gram = 3, max_length = 100, batch_size = 49152):\n",
    "    hashed_vocab = vocab_to_hash(vocab_path = vocab_path,\n",
    "                            HASH_SPACE_DIM = HASH_SPACE_DIM)\n",
    "    ngrams_list = list(hashed_vocab.keys())  # 获取所有 n-gram 作为列表\n",
    "    indices_list = list(hashed_vocab.values())  # 获取对应的哈希索引列表\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    # 将 n-grams 和索引转化为 GPU 上的张量\n",
    "    ngrams_tensor = torch.tensor([hash(ngram) % HASH_SPACE_DIM for ngram in ngrams_list]).to(\"mps\")\n",
    "    indices_tensor = torch.tensor(indices_list).to(\"mps\")\n",
    "\n",
    "    start_index = 0\n",
    "    all_evd_chars = []\n",
    "    while start_index < len(evid_text):\n",
    "        print(\"current processed batch index\",start_index)\n",
    "        batch_data, start_index = get_batch(evid_text, batch_size, start_index)\n",
    "        batch_char_list = process_batch_char(batch_data, indices_tensor, HASH_SPACE_DIM, n_gram = 3, max_length = 100)\n",
    "        batch_char_tensor_cpu = batch_char_list.to(\"cpu\")\n",
    "        all_evd_chars.append(batch_char_tensor_cpu)\n",
    "    return all_evd_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current processed batch index 0\n",
      "current processed batch index 49152\n",
      "current processed batch index 98304\n",
      "current processed batch index 147456\n",
      "current processed batch index 196608\n",
      "current processed batch index 245760\n",
      "current processed batch index 294912\n",
      "current processed batch index 344064\n",
      "current processed batch index 393216\n",
      "current processed batch index 442368\n",
      "current processed batch index 491520\n"
     ]
    }
   ],
   "source": [
    "all_evd_chars = load_evd_text(vocab_path = './data/vocab_true_evid.txt',evid_text = list(evidence.values()),\n",
    "                            batch_size = 49152,\n",
    "                            HASH_SPACE_DIM = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current processed batch index 0\n",
      "current processed batch index 49152\n",
      "current processed batch index 98304\n",
      "current processed batch index 147456\n",
      "current processed batch index 196608\n",
      "current processed batch index 245760\n",
      "current processed batch index 294912\n",
      "current processed batch index 344064\n",
      "current processed batch index 393216\n",
      "current processed batch index 442368\n",
      "current processed batch index 491520\n"
     ]
    }
   ],
   "source": [
    "start_index = 0\n",
    "batch_size = 49152\n",
    "batched_char_list = []\n",
    "while start_index < len(evid_text):\n",
    "    print(\"current processed batch index\",start_index)\n",
    "    batch_data, start_index = get_batch(evid_text, batch_size, start_index)\n",
    "    batch_char_list = process_batch_char(batch_data, HASH_SPACE_DIM, device, n = 3)\n",
    "    batch_char_tensor_cpu = batch_char_list.to(\"cpu\")\n",
    "    batched_char_list.append(batch_char_tensor_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 将 n-grams 转换为 GPU 张量形式\n",
    "batch_ngrams_tensor = torch.tensor([hash(ngram) % vocab_size for ngram in batch_ngrams]).to(\"mps\")\n",
    "\n",
    "# GPU 加速查找：通过张量查找对应索引\n",
    "hash_indices = indices_tensor[batch_ngrams_tensor]\n",
    "\n",
    "print(\"Hash indices:\", hash_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
