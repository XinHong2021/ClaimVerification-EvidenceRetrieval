{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from statistics import mean\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "pd.set_option('display.float_format', '{:.1f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/Documents/COMP90042_NLP/Project/COMP90042_2024_Project\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/macbookpro/Documents/COMP90042_NLP/Project/COMP90042_2024_Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/train-claims.json\", 'r') as file:\n",
    "    train_claims = json.load(file)\n",
    "\n",
    "with open(\"data/dev-claims.json\", 'r') as file:\n",
    "    dev_claims = json.load(file)\n",
    "\n",
    "with open(\"data/test-claims-unlabelled.json\", 'r') as file:\n",
    "    test_claims_unlabelled = json.load(file)\n",
    "\n",
    "with open(\"data/dev_pred.json\", 'r') as file:\n",
    "    dev_pred = json.load(file)\n",
    "\n",
    "with open(\"data/evidence.json\", 'r') as file:\n",
    "    evidence = json.load(file)\n",
    "\n",
    "with open(\"data/test_pred.json\", 'r') as file:\n",
    "    test_claims_pred = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/replaced_train.json\", 'r') as file:\n",
    "    replaced_train = json.load(file)\n",
    "\n",
    "with open(\"data/replaced_evid.json\", 'r') as file:\n",
    "    replaced_evid = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/external/preprocessed_evidences.json\", 'r') as file:\n",
    "    prepro_evd_texts = json.load(file)\n",
    "\n",
    "prepro_evidence = {}\n",
    "i = 0\n",
    "for key, value in evidence.items():\n",
    "    prepro_evidence[key] = \" \".join(prepro_evd_texts[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(columns=['claim_id','text'])\n",
    "train_y = []\n",
    "for key, value in train_claims.items():\n",
    "    row_to_add = pd.DataFrame({\n",
    "        \"claim_id\" :[key],\n",
    "        \"text\": [\"<sep>\".join([\"<cls>\" +value['claim_text']] + [prepro_evidence[evd_id] for evd_id in value['evidences']])]\n",
    "    })\n",
    "    train_df = train_df.append(row_to_add, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"SUPPORTS\", \"REFUTES\", \"NOT_ENOUGH_INFO\", \"DISPUTED\"]\n",
    "label_map = {\"SUPPORTS\":0, \"REFUTES\":1, \"NOT_ENOUGH_INFO\":2, \"DISPUTED\":3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_data(claims,evidence):\n",
    "    X = pd.DataFrame(columns=['claim_id','text','num_evidences','evidence_ids'])\n",
    "    Y = []\n",
    "    # prepare text: <cls> claim text <sep> evidence_1 text <sep> evidence_2 text\n",
    "    for key, value in claims.items():\n",
    "        row_to_add = pd.DataFrame({\n",
    "            \"claim_id\" :[key],\n",
    "            \"text\": [\"<sep>\".join([\"<cls>\" +value['claim_text']] + [evidence[evd_id] for evd_id in value['evidences']])],\n",
    "            'num_evidences': len(value['evidences']),\n",
    "            'evidence_ids': [value['evidences']]\n",
    "        })\n",
    "        Y.append(value['claim_label'])\n",
    "        X = X.append(row_to_add, ignore_index = True)\n",
    "    X_list = X['text'].values\n",
    "    Y = list(map(lambda x: label_map[x], Y))\n",
    "    return X, X_list, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<cls>In 1946, PDO switched to a cool phase.<sep>There is evidence of reversals in the prevailing polarity (meaning changes in cool surface waters versus warm surface waters within the region) of the oscillation occurring around 1925, 1947, and 1977; the last two reversals corresponded with dramatic shifts in salmon production regimes in the North Pacific Ocean.<sep>1945/1946: The PDO changed to a \"cool\" phase, the pattern of this regime shift is similar to the 1970s episode with maximum amplitude in the subarctic and subtropical front but with a greater signature near the Japan while the 1970s shift was stronger near the American west coast.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"<sep>\".join([\"<cls>\" +train_claims['claim-2510']['claim_text']] + [evidence[evd_id] for evd_id in train_claims['claim-2510']['evidences']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_X_only(claims, evidence):\n",
    "    X = pd.DataFrame(columns=['claim_id','text','num_evidences','evidence_ids'])\n",
    "\n",
    "    # prepare text: <cls> claim text <sep> evidence_1 text <sep> evidence_2 text\n",
    "    for key, value in claims.items():\n",
    "        row_to_add = pd.DataFrame({\n",
    "            \"claim_id\" :[key],\n",
    "            \"text\": [\"<sep>\".join([\"<cls>\" +value['claim_text']] + [evidence[evd_id] for evd_id in value['evidences']])],\n",
    "            'num_evidences': len(value['evidences']),\n",
    "            'evidence_ids': [value['evidences']]\n",
    "        })\n",
    "        X = X.append(row_to_add, ignore_index = True)\n",
    "    X_list = X['text'].values\n",
    "    \n",
    "    return X, X_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, train_X, train_y = reconstruct_data(train_claims,evidence)\n",
    "dev_df_true, dev_X_true, dev_y_true = reconstruct_data(dev_claims,evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'evidences'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gk/b497571d49ndglvgyk5sb53h0000gn/T/ipykernel_3276/3270830309.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdev_df_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_X_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreconstruct_X_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreconstruct_X_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_claims_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/gk/b497571d49ndglvgyk5sb53h0000gn/T/ipykernel_3276/1364878080.py\u001b[0m in \u001b[0;36mreconstruct_X_only\u001b[0;34m(claims, evidence)\u001b[0m\n\u001b[1;32m      6\u001b[0m         row_to_add = pd.DataFrame({\n\u001b[1;32m      7\u001b[0m             \u001b[0;34m\"claim_id\"\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"<sep>\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<cls>\"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'claim_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mevidence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevd_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevd_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'evidences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;34m'num_evidences'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'evidences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;34m'evidence_ids'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'evidences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'evidences'"
     ]
    }
   ],
   "source": [
    "dev_df_pred, dev_X_pred = reconstruct_X_only(dev_pred,evidence)\n",
    "test_df, test_X = reconstruct_X_only(test_claims_pred,evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import torch\n",
    "# from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "train_iter = train_df['text'].tolist()\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=('<unk>', '<pad>'))\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "padding_index = vocab[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenizer, vocabulary=vocab.get_stoi(), lowercase=True)\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizer, vocabulary=vocab.get_stoi(), lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorizer.transform(train_df['text'].values).toarray()\n",
    "x_dev = vectorizer.transform(dev_df_true['text'].values).toarray()\n",
    "x_test = vectorizer.transform(test_df['text'].values).toarray()\n",
    "x_dev_pred = vectorizer.transform(dev_df_pred['text'].values).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 11955\n",
      "['<cls>not', 'only', 'is', 'there', 'no', 'scientific', 'evidence', 'that', 'co2', 'is', 'a', 'pollutant', ',', 'higher', 'co2', 'concentrations', 'actually', 'help', 'ecosystems', 'support', 'more', 'plant', 'and', 'animal', 'life', '.', '<sep>at', 'very', 'high', 'concentrations', '(', '100', 'times', 'atmospheric', 'concentration', ',', 'or', 'greater', ')', ',', 'carbon', 'dioxide', 'can', 'be', 'toxic', 'to', 'animal', 'life', ',', 'so', 'raising', 'the', 'concentration', 'to', '10', ',', '000', 'ppm', '(', '1%', ')', 'or', 'higher', 'for', 'several', 'hours', 'will', 'eliminate', 'pests', 'such', 'as', 'whiteflies', 'and', 'spider', 'mites', 'in', 'a', 'greenhouse', '.', '<sep>plants', 'can', 'grow', 'as', 'much', 'as', '50', 'percent', 'faster', 'in', 'concentrations', 'of', '1', ',', '000', 'ppm', 'co', '2', 'when', 'compared', 'with', 'ambient', 'conditions', ',', 'though', 'this', 'assumes', 'no', 'change', 'in', 'climate', 'and', 'no', 'limitation', 'on', 'other', 'nutrients', '.', '<sep>higher', 'carbon', 'dioxide', 'concentrations', 'will', 'favourably', 'affect', 'plant', 'growth', 'and', 'demand', 'for', 'water', '.']\n",
      "[0 0 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = x_train.shape[1]\n",
    "print(\"Vocab size =\", vocab_size)\n",
    "print(tokenizer(train_df['text'].values[0]))\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_pipeline = lambda x: vocab(tokenizer(x))\n",
    "sequence_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "# device = (\n",
    "#     \"cuda\" if torch.cuda.is_available()\n",
    "#     else \"mps\" if torch.backends.mps.is_available()\n",
    "#     else \"cpu\"\n",
    "# )\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set max length\n",
    "max_len = 150\n",
    "embedding_dim = 64\n",
    "hidden_dim = 64\n",
    "padding_index = vocab[\"<pad>\"]\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def seq_collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for  _text, _label in batch:\n",
    "        label_list.append(_label)\n",
    "        text_list.append(sequence_pipeline(_text))\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.long)\n",
    "    # one_hot_labels = F.one_hot(label_list, num_classes=4)\n",
    "\n",
    "    # Pad or truncate each sequence\n",
    "    padded_sequences = []\n",
    "    for seq in text_list:\n",
    "        # Truncate if longer than max_len\n",
    "        padded_seq = seq[:max_len]\n",
    "         # Pad if shorter\n",
    "        padded_seq += [padding_index] * (max_len - len(padded_seq)) \n",
    "        padded_sequences.append(torch.tensor(padded_seq))\n",
    "\n",
    "    text_list = torch.stack(padded_sequences)\n",
    "\n",
    "    # Stack all sequences into a single tensor\n",
    "    return text_list, label_list.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create data loaders.\n",
    "xseq_train_dataloader = DataLoader(list(zip(train_X, train_y)), batch_size=5, collate_fn=seq_collate_batch)\n",
    "xseq_dev_dataloader = DataLoader(list(zip(dev_X_true, dev_y_true)), batch_size=5, collate_fn=seq_collate_batch)\n",
    "xseq_dev_dataloader_pred = DataLoader(list(zip(dev_X_pred, dev_y_true)), batch_size=5, collate_fn=seq_collate_batch)\n",
    "\n",
    "# xseq_test_dataloader = DataLoader(list(zip(test_X, )), batch_size=10, collate_fn=seq_collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTMNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, padding_idx):\n",
    "        super().__init__()\n",
    "        self.embedding  = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm_layer = nn.LSTM(embedding_dim, hidden_dim,num_layers=3, batch_first=True)\n",
    "\n",
    "        self.forward_layer = nn.Linear(hidden_dim, 4)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        h0 = torch.zeros((3, batch_size, hidden_dim))#.to(device)\n",
    "        c0 = torch.zeros((3, batch_size, hidden_dim))#.to(device)\n",
    "        hidden = (h0, c0)\n",
    "        return hidden\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = [batch size, seq length]\n",
    "\n",
    "        embedded = self.embedding(x)\n",
    "        # embedded = [batch size, seq length, emb dim]\n",
    "\n",
    "        h0 = self.init_hidden(x.shape[0])\n",
    "\n",
    "        output, (hidden, cell) = self.lstm_layer(embedded, h0)\n",
    "        # output = [batch size, seq length, hid dim * num directions]\n",
    "        # hidden = [num layers * num directions, batch size, hid dim]\n",
    "        # cell = [num layers * num directions, batch size, hid dim]\n",
    "\n",
    "        hidden = hidden[-1, :, :]\n",
    "        # hidden = [batch size, hid dim]\n",
    "\n",
    "        logits = torch.sigmoid(self.forward_layer(hidden))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulate_step = 2\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    # Calling .train() will evoke the tensor to start caching steps and gradients.\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        # Compute prediction by directly calling the model variable.\n",
    "        pred = model(X)\n",
    "\n",
    "        y = torch.Tensor(y).flatten()\n",
    "        # Calculate the loss by comparing the prediction and the true labels.\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation: calculate the gradient by walking back the cached steps.\n",
    "        loss.backward()\n",
    "        # Update the parameters of the model with the loss gradient.\n",
    "        optimizer.step()\n",
    "        # Remove all the gradient to be ready for the next training of the next batch.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch  == size - 1:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"Train loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    # After calling eval(), the model no longer caching steps and gradient.\n",
    "    # The model also does the inference faster with less resource.\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    # This line specify that there will be no gradient in the operation below.\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            y = torch.Tensor(y).flatten()\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "            result = torch.argmax(pred,dim=1)\n",
    "            correct += (result == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM network model!\n",
      "SimpleLSTMNetwork(\n",
      "  (embedding): Embedding(11955, 64, padding_idx=1)\n",
      "  (lstm_layer): LSTM(64, 64, num_layers=5, batch_first=True)\n",
      "  (forward_layer): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 17.5%, Avg loss: 1.517900 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 18.2%, Avg loss: 1.519660 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 18.2%, Avg loss: 1.533606 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 29.9%, Avg loss: 1.529468 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 44.2%, Avg loss: 1.435983 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.387582 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.389148 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.389456 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.389587 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.389657 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.389701 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.389730 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.389752 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.389769 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.389782 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.389793 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.389802 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.389810 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.389817 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.389823 \n",
      "\n",
      "Done!\n",
      "final test:\n"
     ]
    }
   ],
   "source": [
    "print(\"Training LSTM network model!\")\n",
    "\n",
    "lstm_model = SimpleLSTMNetwork(vocab_size, embedding_dim, padding_index)\n",
    "print(lstm_model)\n",
    "\n",
    "# loss_fn = nn.BCELoss()\n",
    "loss_fn = nn.CrossEntropyLoss(torch.FloatTensor([0.2, 0.5, 0.3, 1.]))\n",
    "\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "    train(xseq_train_dataloader, lstm_model, loss_fn, optimizer)\n",
    "    test(xseq_dev_dataloader_pred, lstm_model, loss_fn)\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"final test:\")\n",
    "# test(xseq_test_dataloader, lstm_model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer = 3, test error: 36.4%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
