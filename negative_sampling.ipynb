{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "from make_vocab import lst_gram, n_gram\n",
    "import json\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/'\n",
    "files = ['norm_train_claims.json', 'norm_dev_claims.json', 'norm_climate_evidences.json', 'evidence.json']\n",
    "\n",
    "for file in files:\n",
    "    with open(file_path + file, 'r', encoding='utf-8') as f:\n",
    "        if file == 'norm_train_claims.json':\n",
    "            train_claims = json.load(f)\n",
    "        elif file == 'norm_dev_claims.json':\n",
    "            dev_claims = json.load(f)\n",
    "        elif file == 'norm_climate_evidences.json':\n",
    "            evidences = json.load(f)\n",
    "        elif file == 'evidence.json':\n",
    "            evidences_all = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evid_text_list = list(evidences.values())\n",
    "train_text_list = []\n",
    "for key, values in train_claims.items():\n",
    "    train_text_list.append(values['norm_claim'])\n",
    "len(train_text_list)\n",
    "dev_text_list = []\n",
    "for key, values in dev_claims.items():\n",
    "    dev_text_list.append(values['norm_claim'])\n",
    "len(dev_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(521503, 445812)\n",
      "(1228, 445812)\n",
      "(154, 445812)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "vectorizer.fit(evid_text_list+train_text_list)\n",
    "evid_tfidf = vectorizer.transform(evid_text_list)\n",
    "train_tfidf = vectorizer.transform(train_text_list)\n",
    "dev_tfidf = vectorizer.transform(dev_text_list)\n",
    "print(evid_tfidf.shape), print(train_tfidf.shape), print(dev_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1228, 521503)\n"
     ]
    }
   ],
   "source": [
    "con_sim = cosine_similarity(train_tfidf, evid_tfidf)\n",
    "print(con_sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1228, 100)\n"
     ]
    }
   ],
   "source": [
    "top_k_similarities = np.argsort(con_sim, axis=1)[:,-100:]\n",
    "print(top_k_similarities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "evid_ids_list = list(evidences.keys())\n",
    "\n",
    "def top_k_similarities(claim_tfidf,evid_tfidf, k):\n",
    "    con_sim = cosine_similarity(claim_tfidf, evid_tfidf)\n",
    "    ranked_evd_id, ranked_evd_score = [], []\n",
    "\n",
    "    for i in range(con_sim.shape[0]):\n",
    "        # for each claim\n",
    "        cos_sim_claim = con_sim[i]\n",
    "        # find top k cos similarity\n",
    "        top_evd_index = np.argsort(cos_sim_claim).tolist()[-k:][::-1]\n",
    "        top_evd_ids = [evid_ids_list[i] for i in top_evd_index]\n",
    "        top_evd_score = np.sort(cos_sim_claim).tolist()[-k:][::-1]\n",
    "        # append the top k evidence list\n",
    "        ranked_evd_id.append(top_evd_ids)\n",
    "        ranked_evd_score.append(top_evd_score)\n",
    "    return ranked_evd_id, ranked_evd_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_evd_id_train, top_evd_score_train = top_k_similarities(train_tfidf, evid_tfidf, 2000)\n",
    "# top_evd_id_dev, top_evd_score_dev = top_k_similarities(dev_tfidf, evid_tfidf, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(gt_evid_ids, pred_evid_ids):\n",
    "    recall = 0\n",
    "    for i in range(len(gt_evid_ids)):\n",
    "        if gt_evid_ids[i] in pred_evid_ids:\n",
    "            recall += 1\n",
    "    return recall/len(gt_evid_ids)\n",
    "\n",
    "def avg_recall(gt_claims, pred_claims):\n",
    "    avg_recall = 0\n",
    "    # for claim_id, values in gt_claims.items():\n",
    "    for i, values in enumerate(gt_claims):\n",
    "        avg_recall += recall(gt_claims[values]['evidences'], pred_claims[i])\n",
    "    return avg_recall/len(gt_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5776465798045601\n",
      "0.6642100977198694\n",
      "0.7125135722041248\n",
      "0.7397258414766548\n",
      "0.778732356134635\n"
     ]
    }
   ],
   "source": [
    "top_k = [500, 1000, 1500, 2000, 3000]\n",
    "for k in top_k:\n",
    "    top_evd_id_train, top_evd_score_train = top_k_similarities(train_tfidf, evid_tfidf, k)\n",
    "    rec = avg_recall(train_claims, top_evd_id_train)\n",
    "    print(k, rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 0.8062839305103133\n",
      "5000 0.8255293159609101\n",
      "6000 0.8398208469055354\n"
     ]
    }
   ],
   "source": [
    "top_k = [4000, 5000, 6000]\n",
    "for k in top_k:\n",
    "    top_evd_id_train, top_evd_score_train = top_k_similarities(train_tfidf, evid_tfidf, k)\n",
    "    rec = avg_recall(train_claims, top_evd_id_train)\n",
    "    print(k, rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sample(i, values, top_evd_id_train, claim_df, evids_pool, num_hard, num_easy):\n",
    "    pred_claim_evids = top_evd_id_train[i]\n",
    "    gt_claim_evids = claim_df[values]['evidences']\n",
    "\n",
    "    easy_pool = [evd for evd in evids_pool if evd not in pred_claim_evids and evd not in gt_claim_evids]\n",
    "    easy_neg = random.sample(easy_pool, num_easy)\n",
    "    \n",
    "    hard_neg = []\n",
    "    idx, eff_idx = 0, 0\n",
    "    while eff_idx < num_hard:\n",
    "        hard_id = pred_claim_evids[idx]\n",
    "        if hard_id not in gt_claim_evids:\n",
    "            hard_neg.append(hard_id)\n",
    "            eff_idx += 1\n",
    "        idx += 1\n",
    "    neg_samples = hard_neg + easy_neg\n",
    "    return neg_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_neg(top_evd_ids, claim_df, evids_pool, num_hard, num_easy):\n",
    "    for i, values in enumerate(claim_df):\n",
    "        if i % 200 == 0:\n",
    "            print(f\"{i} claims have been processed.\")\n",
    "        neg_samples = negative_sample(i, values, top_evd_ids, claim_df, evids_pool, num_hard, num_easy)\n",
    "\n",
    "        claim_df[values]['neg_evidences'] = neg_samples\n",
    "    return claim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_evd_id_train, top_evd_score_train = top_k_similarities(train_tfidf, evid_tfidf, 10)\n",
    "copied_train = copy.deepcopy(train_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 claims have been processed.\n",
      "200 claims have been processed.\n",
      "400 claims have been processed.\n",
      "600 claims have been processed.\n",
      "800 claims have been processed.\n",
      "1000 claims have been processed.\n",
      "1200 claims have been processed.\n"
     ]
    }
   ],
   "source": [
    "train_with_neg = add_neg(top_evd_id_train, copied_train, evid_ids_list, 3, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 claims have been processed.\n"
     ]
    }
   ],
   "source": [
    "top_evd_id_dev, top_evd_score_dev = top_k_similarities(dev_tfidf, evid_tfidf, 10)\n",
    "copied_dev = copy.deepcopy(dev_claims)\n",
    "dev_with_neg = add_neg(top_evd_id_dev, copied_dev, evid_ids_list, 3, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/output/train_with_neg_303.json\", \"w\") as f:\n",
    "    json.dump(train_with_neg, f)\n",
    "with open(\"data/output/dev_with_neg_303.json\", \"w\") as f:\n",
    "    json.dump(dev_with_neg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
